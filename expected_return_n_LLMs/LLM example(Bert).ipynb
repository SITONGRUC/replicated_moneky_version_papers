{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## BERT\n",
        "#### Since this method requres time. I could not test it with big data\n",
        "#### the code I show below could run on the same dataset I used for BoW method. The only thing needed is training \n",
        "#### And Prof you could see from the print out, which shows that only 96 data volume require 10 min for one epoch. \n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [],
      "source": [
        "import torch\n",
        "from transformers import AutoModel, AutoTokenizer\n",
        "from torch.optim import Adam\n",
        "from tqdm import tqdm\n",
        "import os\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import statsmodels.api as sm\n",
        "import matplotlib.pyplot as plt\n",
        "from pandas.tseries.offsets import MonthEnd\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [],
      "source": [
        "data_news = pd.read_csv(\"AAPL.csv\", error_bad_lines=False,delimiter = \"\\t\")\n",
        "data_price = pd.read_csv('AAPL_price.csv')\n",
        "data_price = data_price[['Date','Adj Close']].copy()\n",
        "data_price['lead'] = data_price['Adj Close'].shift(-1)\n",
        "data_price['lag'] = data_price['Adj Close'].shift(1)\n",
        "data_price['label'] = data_price['lead'] > data_price['lag']\n",
        "data_price['label'] = data_price['label'].astype(int)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [],
      "source": [
        "data_price = data_price[['Date','label']]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [],
      "source": [
        "data_price['Date'] = pd.to_datetime(data_price['Date'])\n",
        "data_news['date'] = pd.to_datetime(data_news['date'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {},
      "outputs": [],
      "source": [
        "data = pd.merge(data_news,\n",
        "                data_price[['Date','label']].rename(columns = {'Date':'date'}),\n",
        "                on = 'date',\n",
        "                how = 'left'\n",
        "                \n",
        "                )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>date</th>\n",
              "      <th>time</th>\n",
              "      <th>ticker</th>\n",
              "      <th>name</th>\n",
              "      <th>title</th>\n",
              "      <th>summary</th>\n",
              "      <th>link</th>\n",
              "      <th>label</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>2021-12-19</td>\n",
              "      <td>2021-12-19 04:34:20</td>\n",
              "      <td>AAPL</td>\n",
              "      <td>Apple Inc.</td>\n",
              "      <td>Exclusive-Apple seeks dismissal of India apps ...</td>\n",
              "      <td>Apple Inc has asked India's antitrust watchdog...</td>\n",
              "      <td>https://finance.yahoo.com/news/exclusive-apple...</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>2021-12-19</td>\n",
              "      <td>2021-12-19 01:25:39</td>\n",
              "      <td>AAPL</td>\n",
              "      <td>Apple Inc.</td>\n",
              "      <td>Dow Jones Futures: Why This Market Rally Is So...</td>\n",
              "      <td>The market rally is chopping up investors fina...</td>\n",
              "      <td>https://finance.yahoo.com/m/b19de6ca-ffa2-3872...</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>2021-12-18</td>\n",
              "      <td>2021-12-18 16:15:00</td>\n",
              "      <td>AAPL</td>\n",
              "      <td>Apple Inc.</td>\n",
              "      <td>3 Top Energy Stocks Ready for a Bull Run</td>\n",
              "      <td>Decarbonization might be one of the biggest me...</td>\n",
              "      <td>https://finance.yahoo.com/m/1edd438b-e080-3f5e...</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>2021-12-18</td>\n",
              "      <td>2021-12-18 14:22:14</td>\n",
              "      <td>AAPL</td>\n",
              "      <td>Apple Inc.</td>\n",
              "      <td>Why Bruce Springsteen's $500M deal signals a '...</td>\n",
              "      <td>\"The Boss\" is cashing in — and that could have...</td>\n",
              "      <td>https://finance.yahoo.com/news/why-bruce-sprin...</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>2021-12-18</td>\n",
              "      <td>2021-12-18 12:05:00</td>\n",
              "      <td>AAPL</td>\n",
              "      <td>Apple Inc.</td>\n",
              "      <td>3 Stocks That Could Be Worth More Than Apple b...</td>\n",
              "      <td>Apple (NASDAQ: AAPL) is currently the world's ...</td>\n",
              "      <td>https://finance.yahoo.com/m/d3ac17b8-2954-37c8...</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>58684</th>\n",
              "      <td>2018-01-01</td>\n",
              "      <td>2018-01-01 23:28:00</td>\n",
              "      <td>AAPL</td>\n",
              "      <td>Apple Inc.</td>\n",
              "      <td>Forget Your iPhone X, Ignore The Samsung And P...</td>\n",
              "      <td>Looking back on 2017, the smartphone industry ...</td>\n",
              "      <td>http://finance.yahoo.com/r/2e9f187e-56ab-319e-...</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>58685</th>\n",
              "      <td>2018-01-01</td>\n",
              "      <td>2018-01-01 22:19:00</td>\n",
              "      <td>AAPL</td>\n",
              "      <td>Apple Inc.</td>\n",
              "      <td>Apple's Executive Cash Bonus Plan</td>\n",
              "      <td>Apple’s executives have done very well over th...</td>\n",
              "      <td>http://finance.yahoo.com/r/a212bfe1-4a00-3d00-...</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>58686</th>\n",
              "      <td>2018-01-01</td>\n",
              "      <td>2018-01-01 21:08:20</td>\n",
              "      <td>AAPL</td>\n",
              "      <td>Apple Inc.</td>\n",
              "      <td>2 Warren Buffett Stocks to Consider Buying Now</td>\n",
              "      <td>A 1 000 investment in Berkshire Hathaway 160 s...</td>\n",
              "      <td>http://articlefeeds.nasdaq.com/~r/nasdaq/symbo...</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>58687</th>\n",
              "      <td>2018-01-01</td>\n",
              "      <td>2018-01-01 20:02:00</td>\n",
              "      <td>AAPL</td>\n",
              "      <td>Apple Inc.</td>\n",
              "      <td>2 Warren Buffett Stocks to Consider Buying Now</td>\n",
              "      <td>The legendary investor knows a thing or two ab...</td>\n",
              "      <td>https://finance.yahoo.com/news/2-warren-buffet...</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>58688</th>\n",
              "      <td>2018-01-01</td>\n",
              "      <td>2018-01-01 16:06:40</td>\n",
              "      <td>AAPL</td>\n",
              "      <td>Apple Inc.</td>\n",
              "      <td>Bitcoin or Stocks? Here’s the One to Buy in 2018</td>\n",
              "      <td>By Michael Foster Worried that a bursting bitc...</td>\n",
              "      <td>http://articlefeeds.nasdaq.com/~r/nasdaq/symbo...</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>58689 rows × 8 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "            date                 time ticker        name  \\\n",
              "0     2021-12-19  2021-12-19 04:34:20   AAPL  Apple Inc.   \n",
              "1     2021-12-19  2021-12-19 01:25:39   AAPL  Apple Inc.   \n",
              "2     2021-12-18  2021-12-18 16:15:00   AAPL  Apple Inc.   \n",
              "3     2021-12-18  2021-12-18 14:22:14   AAPL  Apple Inc.   \n",
              "4     2021-12-18  2021-12-18 12:05:00   AAPL  Apple Inc.   \n",
              "...          ...                  ...    ...         ...   \n",
              "58684 2018-01-01  2018-01-01 23:28:00   AAPL  Apple Inc.   \n",
              "58685 2018-01-01  2018-01-01 22:19:00   AAPL  Apple Inc.   \n",
              "58686 2018-01-01  2018-01-01 21:08:20   AAPL  Apple Inc.   \n",
              "58687 2018-01-01  2018-01-01 20:02:00   AAPL  Apple Inc.   \n",
              "58688 2018-01-01  2018-01-01 16:06:40   AAPL  Apple Inc.   \n",
              "\n",
              "                                                   title  \\\n",
              "0      Exclusive-Apple seeks dismissal of India apps ...   \n",
              "1      Dow Jones Futures: Why This Market Rally Is So...   \n",
              "2               3 Top Energy Stocks Ready for a Bull Run   \n",
              "3      Why Bruce Springsteen's $500M deal signals a '...   \n",
              "4      3 Stocks That Could Be Worth More Than Apple b...   \n",
              "...                                                  ...   \n",
              "58684  Forget Your iPhone X, Ignore The Samsung And P...   \n",
              "58685                  Apple's Executive Cash Bonus Plan   \n",
              "58686     2 Warren Buffett Stocks to Consider Buying Now   \n",
              "58687     2 Warren Buffett Stocks to Consider Buying Now   \n",
              "58688   Bitcoin or Stocks? Here’s the One to Buy in 2018   \n",
              "\n",
              "                                                 summary  \\\n",
              "0      Apple Inc has asked India's antitrust watchdog...   \n",
              "1      The market rally is chopping up investors fina...   \n",
              "2      Decarbonization might be one of the biggest me...   \n",
              "3      \"The Boss\" is cashing in — and that could have...   \n",
              "4      Apple (NASDAQ: AAPL) is currently the world's ...   \n",
              "...                                                  ...   \n",
              "58684  Looking back on 2017, the smartphone industry ...   \n",
              "58685  Apple’s executives have done very well over th...   \n",
              "58686  A 1 000 investment in Berkshire Hathaway 160 s...   \n",
              "58687  The legendary investor knows a thing or two ab...   \n",
              "58688  By Michael Foster Worried that a bursting bitc...   \n",
              "\n",
              "                                                    link  label  \n",
              "0      https://finance.yahoo.com/news/exclusive-apple...    NaN  \n",
              "1      https://finance.yahoo.com/m/b19de6ca-ffa2-3872...    NaN  \n",
              "2      https://finance.yahoo.com/m/1edd438b-e080-3f5e...    NaN  \n",
              "3      https://finance.yahoo.com/news/why-bruce-sprin...    NaN  \n",
              "4      https://finance.yahoo.com/m/d3ac17b8-2954-37c8...    NaN  \n",
              "...                                                  ...    ...  \n",
              "58684  http://finance.yahoo.com/r/2e9f187e-56ab-319e-...    NaN  \n",
              "58685  http://finance.yahoo.com/r/a212bfe1-4a00-3d00-...    NaN  \n",
              "58686  http://articlefeeds.nasdaq.com/~r/nasdaq/symbo...    NaN  \n",
              "58687  https://finance.yahoo.com/news/2-warren-buffet...    NaN  \n",
              "58688  http://articlefeeds.nasdaq.com/~r/nasdaq/symbo...    NaN  \n",
              "\n",
              "[58689 rows x 8 columns]"
            ]
          },
          "execution_count": 13,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [],
      "source": [
        "data_trail = data.dropna().head(160)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [],
      "source": [
        "model_name = 'bert-base-uncased'\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {},
      "outputs": [],
      "source": [
        "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "model = AutoModel.from_pretrained(model_name)\n",
        "# example_text = 'I will watch Memento tonight'\n",
        "# bert_input = tokenizer(example_text,padding='max_length', \n",
        "#                        max_length = 512, \n",
        "#                        truncation=True,\n",
        "#                        return_tensors=\"pt\")\n",
        "#print(bert_input['input_ids'])\n",
        "# print(bert_input['token_type_ids'])\n",
        "# print(bert_input['attention_mask'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {},
      "outputs": [],
      "source": [
        "class Dataset(torch.utils.data.Dataset):\n",
        "    def __init__(self,data_trail ):\n",
        "        self.labels = [data_trail['label'].iloc[i] for i in range(len(data_trail))]\n",
        "        self.texts = [tokenizer(data_trail['title'].iloc[i], \n",
        "                                padding='max_length', \n",
        "                                max_length = 512, \n",
        "                                truncation=True,\n",
        "                                return_tensors=\"pt\") \n",
        "                      for i in range(len(data_trail))]\n",
        "\n",
        "    def classes(self):\n",
        "        return self.labels\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.labels)\n",
        "\n",
        "    def get_batch_labels(self, idx):\n",
        "        # Fetch a batch of labels\n",
        "        return np.array(self.labels[idx])\n",
        "\n",
        "    def get_batch_texts(self, idx):\n",
        "        # Fetch a batch of inputs\n",
        "        return self.texts[idx]\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        batch_texts = self.get_batch_texts(idx)\n",
        "        batch_y = self.get_batch_labels(idx)\n",
        "        return batch_texts, batch_y"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "96 32 32\n"
          ]
        }
      ],
      "source": [
        "df_train, df_val, df_test = np.split(data_trail.sample(frac=1, random_state=42), \n",
        "                                     [int(.6*len(data_trail)), int(.8*len(data_trail))])\n",
        "\n",
        "print(len(df_train),len(df_val), len(df_test))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {},
      "outputs": [],
      "source": [
        "from torch import nn\n",
        "from transformers import BertModel\n",
        "\n",
        "class BertClassifier(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(BertClassifier, self).__init__()\n",
        "        self.bert = AutoModel.from_pretrained(model_name)\n",
        "        self.linear = nn.Linear(768, 1)\n",
        "        self.Sigmoid = nn.Sigmoid()\n",
        "\n",
        "    def forward(self, input_id, mask):\n",
        "        _, pooled_output = self.bert(input_ids= input_id, attention_mask=mask,return_dict=False)\n",
        "        linear_output = self.linear(pooled_output)\n",
        "        final_layer = self.Sigmoid(linear_output)\n",
        "        return final_layer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {},
      "outputs": [],
      "source": [
        "train_data = df_train\n",
        "val_data = df_val\n",
        "epochs = 5\n",
        "model = BertClassifier()\n",
        "learning_rate = 1e-6"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 3/3 [09:17<00:00, 185.93s/it]\n",
            "  0%|          | 0/3 [00:00<?, ?it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epochs: 1 \n",
            "            | Train Loss:  0.020 \n",
            "            | Train Accuracy:  0.000 \n",
            "            | Val Loss:  0.019 \n",
            "            | Val Accuracy:  0.000\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 3/3 [09:34<00:00, 191.61s/it]\n",
            "  0%|          | 0/3 [00:00<?, ?it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epochs: 2 \n",
            "            | Train Loss:  0.020 \n",
            "            | Train Accuracy:  0.000 \n",
            "            | Val Loss:  0.019 \n",
            "            | Val Accuracy:  0.000\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 3/3 [09:24<00:00, 188.21s/it]\n",
            "  0%|          | 0/3 [00:00<?, ?it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epochs: 3 \n",
            "            | Train Loss:  0.020 \n",
            "            | Train Accuracy:  0.000 \n",
            "            | Val Loss:  0.019 \n",
            "            | Val Accuracy:  0.000\n"
          ]
        }
      ],
      "source": [
        "\n",
        "\n",
        "train, val = Dataset(train_data), Dataset(val_data)\n",
        "\n",
        "train_dataloader = torch.utils.data.DataLoader(train, batch_size=40, shuffle=True)\n",
        "val_dataloader = torch.utils.data.DataLoader(val, batch_size=40)\n",
        "\n",
        "use_cuda = torch.cuda.is_available()\n",
        "device = torch.device(\"cuda\" if use_cuda else \"cpu\")\n",
        "\n",
        "criterion = nn.BCELoss()\n",
        "optimizer = Adam(model.parameters(), lr=learning_rate)\n",
        "\n",
        "if use_cuda:\n",
        "        model = model.cuda()\n",
        "        criterion = criterion.cuda()\n",
        "\n",
        "for epoch_num in range(epochs):\n",
        "\n",
        "        total_acc_train = 0\n",
        "        total_loss_train = 0\n",
        "\n",
        "        for train_input, train_label in tqdm(train_dataloader):\n",
        "\n",
        "            train_label = train_label.to(device)\n",
        "            mask = train_input['attention_mask'].to(device)\n",
        "            input_id = train_input['input_ids'].squeeze(1).to(device)\n",
        "\n",
        "            output = model(input_id, mask)\n",
        "\n",
        "            output = output.flatten()\n",
        "            #train_label = train_label.long()\n",
        "            #output = output.double()\n",
        "            train_label = train_label.double()\n",
        "            output = output.double()\n",
        "\n",
        "            batch_loss = criterion(output, train_label)\n",
        "            total_loss_train += batch_loss.item()\n",
        "\n",
        "            acc = (output == train_label).sum().item()\n",
        "            total_acc_train += acc\n",
        "\n",
        "            model.zero_grad()\n",
        "            batch_loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "        total_acc_val = 0\n",
        "        total_loss_val = 0\n",
        "\n",
        "        with torch.no_grad():\n",
        "\n",
        "            for val_input, val_label in val_dataloader:\n",
        "\n",
        "                val_label = val_label.to(device)\n",
        "                mask = val_input['attention_mask'].to(device)\n",
        "                input_id = val_input['input_ids'].squeeze(1).to(device)\n",
        "\n",
        "                output = model(input_id, mask)\n",
        "                output = output.flatten()\n",
        "                #train_label = train_label.long()\n",
        "                #output = output.double()\n",
        "                val_label = val_label.double()\n",
        "                output = output.double()\n",
        "                batch_loss = criterion(output, val_label)\n",
        "                total_loss_val += batch_loss.item()\n",
        "                \n",
        "                acc = (output == val_label).sum().item()\n",
        "                total_acc_val += acc\n",
        "        \n",
        "        print(\n",
        "            f'''Epochs: {epoch_num + 1} \n",
        "            | Train Loss: {total_loss_train / len(train_data): .3f} \n",
        "            | Train Accuracy: {total_acc_train / len(train_data): .3f} \n",
        "            | Val Loss: {total_loss_val / len(val_data): .3f} \n",
        "            | Val Accuracy: {total_acc_val / len(val_data): .3f}''')  "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
